%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[12pt,conference]{ieeeconf} %Github
%\documentclass[letterpaper, 12 pt, onecolumn]{ieeeconf} %Prof. Parallel

% Comment this line out
                                                          % if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed

\usepackage{tikz}
\usetikzlibrary{shapes, arrows.meta, positioning}

\usepackage{url}
\usepackage[ruled, vlined, linesnumbered]{algorithm2e}
%\usepackage{algorithm}
\usepackage{verbatim} 
%\usepackage[noend]{algpseudocode}
\usepackage{soul, color}
\usepackage{lmodern}
\usepackage[hidelinks]{hyperref}
\usepackage{fancyhdr}
\usepackage[utf8]{inputenc}
\usepackage{fourier} 
\usepackage{array}
\usepackage{pgf}
\usepackage{makecell}
\usepackage[sorting=none]{biblatex} % For biblatex
\addbibresource{reference.bib} % Path to your .bib file

\SetNlSty{large}{}{:}

\renewcommand\theadalign{bc}
\renewcommand\theadfont{\bfseries}
\renewcommand\theadgape{\Gape[4pt]}
\renewcommand\cellgape{\Gape[4pt]}

\newcommand{\rework}[1]{\todo[color=yellow,inline]{#1}}

\makeatletter
\newcommand{\rom}[1]{\romannumeral #1}
\newcommand{\Rom}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother

\pagestyle{plain} 

\title{GATTO: Can Topological Information Improve Node Classification via GAT?\\
\large Final Report for Learning from Network's project \\}

\author{Francesco Biscaccia Carrara \textit{(2120934)}, Riccardo Modolo \textit{(2123750)},\\ Alessandro Viespoli \textit{(2120824)} % <-this % stops a space 
\\\\ Master Degree in Computer Engineering \\
University of Padova \\
}

\begin{document}

\maketitle
\thispagestyle{plain}
\pagestyle{plain}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
    In this report we study how topological features of a node can affect its prediction on GAT (Graph Attention Network).
We introduce the dataset, the type of feature we computed and we compare our result with the 
original GAT paper.
    
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION} 
Node classification is an important reasearch and business topic.
Our framework GATTO (Graph ATtention network with TOpological information) want to 
improve the classic GAT prediction using node feature coming from the graph or from its 
embedding. The main idea is to using topological features to imporve prediction of GAT, where each node already have features in it. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{DATA} 
We're going to use the same dataset of GAT$^\text{\cite{GAT}}$, with features to each node.
the Dataset of GAT are: \textit{Cora and Citeseer}. Each node have only one label and the graph is mix directed/undirected
\\
\begin{table}[h!]
    \centering
    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{|l|c|c|c|c|}
    \hline
    \textbf{Network} & \textbf{Nodes} & \textbf{Edges} & \textbf{labels} & \textbf{features} \\
    \hline
    Cora           & 2708 & 5429  & 7  & 1443  \\
    Citeseer       & 3327 & 4732  & 6  & 3703  \\
    \hline
    \end{tabular}
\end{table}
\\
The feature we intended to compute and assign to each node for all these graphs are:
\begin{itemize}
    \item degree centrality
    \item betweenness centrality
    \item closeness centrality
    \item suggested label
\end{itemize}
The \textit{suggested label} parameter is the result of a clustering made on the embedding of the Graph.
We use Node2Vec$^\text{\cite{node2vec}}$ to produce the embedding, and for clustering we use k-mean++ method.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{IMPLEMENTATION} 
The practical implementation$^{\text{\cite{GATTO}}}$ respect the nature of the the framework concept.
We have two block:
\begin{itemize}
    \item{\textbf{Precomputation Module}: the class that compute every needed or requested features from the graph or from it's embedding, and return it as feature matrix}
    \item{\textbf{GAT Module}: the GAT implementation for train and predict node labels}
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{TESTS} 
Since we want to compare the performance of GATTO with respect to GAT baseline algorithm, for each dataset, we perform 10 runs for each technique, to estimate the following parameter:
\begin{itemize}
    \item \textbf{Accuracy}: The proportion of correctly predicted instances (both true positives and true negatives) out of the total instances.
    \item \textbf{Precision}: The proportion of true positive predictions out of all positive predictions. It measures exactness.
    \item \textbf{Recall}: The proportion of true positive predictions out of all actual positive cases. It measures completeness.
    \item \textbf{F1 Score}: The harmonic mean of precision and recall, balancing the two.
\end{itemize}
The algorithms have been executed on 2 different machines: \textit{CAPRI High-Performance Computing} to perform the feature matrix extraction and a general-purpose computer with \textit{Nvidia RTX 2060} to train GAT and analyze its performance.\\
To accomplish such comparison, we utilize a set of statistical tests:
\begin{enumerate}
    \item Since all the scores are values in $[0,1]$, we utilize the Shapiro-Wilk Test to assert the normality of the scores.
    \item If the test "passes", we perform:
        \begin{itemize}
            \item Two-Sample t-Test, with the assumption that the variances are equal
            \item Two-Sample t-Test, with the assumption that the variances are different
            \item Wilcoxon Signed-Rank Test
        \end{itemize}
    \item If the test "fails", we only utilize the Wilcoxon Signed-Rank Test, since we have hard evidence that we are not in a normal scenario. 
\end{enumerate}
The described tests are implemented and available on Github repository as "Results.R" file in the section "code/results".\\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{RESULTS} 
The following tables show the averages and the variances of each score, for each algorithm, based on the test described before.
\begin{table}[h!]
    \centering
    \begin{tabular}{|l|c|c|c|c|} 
    \hline
     & Accuracy & Precision & Recall & F1 Score \\ \hline
    GAT &$0.888$ &$0.891$ &$0.888$ &$0.888$ \\ \hline
    GATTO &$0.890$ &$0.893$ &$0.890$ &$0.890$\\ \hline
    \end{tabular}
    \caption{Averages of the scores for GAT and GATTO (on Cora dataset)}
\end{table}

\begin{table}[h!]
    \centering 
    \begin{tabular}{|l|c|c|c|c|} 
    \hline
     & Accuracy & Precision & Recall & F1 Score \\ \hline
    GAT &$2.566\text{e}-5$ &$2.693\text{e}-5$ &$2.566\text{e}-5$ &$2.564\text{e}-5$ \\ \hline
    GATTO &$1.823\text{e}-5$ &$1.611\text{e}-5$ &$1.823\text{e}-5$ &$1.837\text{e}-5$\\ \hline
    \end{tabular}
    \caption{Variances of the scores for GAT and GATTO (on Cora dataset)}
\end{table}

\begin{table}[h!]
    \centering
    \begin{tabular}{|l|c|c|c|c|} 
    \hline
     & Accuracy & Precision & Recall & F1 Score \\ \hline
    GAT &$0.737$ &$0.732$ &$0.737$ &$0.731$ \\ \hline
    GATTO &$0.736$ &$0.731$ &$0.736$ &$ 0.730$\\ \hline
    \end{tabular}
    \caption{Averages of the scores for GAT and GATTO (on CiteSeer dataset)}
\end{table}

\begin{table}[h!]
    \centering 
    \begin{tabular}{|l|c|c|c|c|} 
    \hline
     & Accuracy & Precision & Recall & F1 Score \\ \hline
    GAT &$2.580\text{e}-5$ &$2.819\text{e}-5$ &$2.580\text{e}-5$ &$2.911\text{e}-5$ \\ \hline
    GATTO &$1.011\text{e}-5$ &$9.231\text{e}-6$ &$1.011\text{e}-5$ &$9.936\text{e}-6$\\ \hline
    \end{tabular}
    \caption{Variances of the scores for GAT and GATTO (on CiteSeer dataset)}
\end{table}


From a naive prospective, it seems that GATTO works a little bit better in terms of both average values and variance values.
However, by running the statistical tests, as explained in the test section, we obtain the tables (link to table). Since we want to bound the Type-I error $\alpha \le 0.05$ and the p-values returned by the tests, we can say that there is no statistically evidence that the algorithm perform equally, at least on small dataset.
This may be due to the fact that adding a small additive "piece of information" to the node (4 features over 1447 for Cora, 4 features over 3707 for CiteSeer) doesn't change the overall perfomance of the GAT network. By such considerations, we can get rid of Precomputation Module since it's computationally expensive and it doesn't provide an actual perfomance improve, even if, in absolute terms, it provides an overall $0.14-0.54\%$ boost.
\begin{table*}[h!]
    \centering 
    \begin{tabular}{|l|c|c|c|} 
    \hline
     & 2S-T-Test (same variance) & 2S-T-Test (diff variance) & Wilcoxon-Test\\ \hline
    Accuracy &$0.546$ &$0.546$ &$0.670$ \\ \hline
    Precision &$0.529$ &$0.530$ &$0.570$\\ \hline
    Recall &$0.546$ &$0.546$ &$0.670$\\ \hline
    F1 Score &$0.524$ &$0.525$ &$0.677$\\ \hline
    \end{tabular}
    \caption{P-values for each test (on Cora dataset)}
\end{table*}

\begin{table*}[h!]
    \centering 
    \begin{tabular}{|l|c|c|c|} 
    \hline
     & 2S-T-Test (same variance) & 2S-T-Test (diff variance) & Wilcoxon-Test\\ \hline
    Accuracy &$\times$ &$\times$ &$0.908$ \\ \hline
    Precision &$0.567$ &$0.569$ &$0.969$\\ \hline
    Recall &$\times$ &$\times$ &$0.908$\\ \hline
    F1 Score &$0.583$ &$0.585$ &$0.969$\\ \hline
    \end{tabular}
    \caption{P-values for each test (on Cora dataset)}
\end{table*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{\fill}
\printbibliography
\newpage
\section*{Work Report}
In this section we want to describe the distribution of the work and detailed contribution of each member.
\begin{itemize}
    \item \textit{Francesco Biscaccia Carrara (2120934)}: {\textbf{40\% of the work}. Produce the code compute features and Runnig test on CAPRI}\\
    \item \textit{Alessandro Viespoli (2120824)}: {\textbf{20\% of the work}. Produce the code for the GAT and the code for plotting results}\\
    \item \textit{Riccardo Modolo (2123750)}: {\textbf{40\% of the work}. Produce the code to retrieve data, review code, write Proposal, Midterm  and final Paper}
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}